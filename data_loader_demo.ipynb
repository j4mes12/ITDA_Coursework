{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "794243c0",
   "metadata": {},
   "source": [
    "# Introduction to Data Analytics Coursework -- Text Analytics Data Loader\n",
    "\n",
    "For this coursework, we recommend that you use your virtual environment that you created for the labs. Alternatively, create a fresh environment following the instructions below. \n",
    "\n",
    "### Setting up your environment\n",
    "\n",
    "We recommend using ```conda``` to create an environment with the correct versions of all the packages you need for these labs. You can install either Anaconda or Miniconda, which will include the ```conda``` program. \n",
    "\n",
    "We provide a .yml file that lists all the packages you will need, and the versions that we have tested the labs with. You can use this file to create your environment as follows.\n",
    "\n",
    "1. Open a terminal. Use the command line to navigate to the directory containing this notebook and the file ```crossplatform_environment.yml```. You can use the command ```cd``` to change directory on the command line.\n",
    "\n",
    "1. If necessary, edit the file ```crossplatform_environment.yml``` to change the name of your new environment. By default, it's called 'data_analytics'. s\n",
    "\n",
    "1. Run conda by typing ```conda env create -f crossplatform_environment.yml```, then answer any questions that appear on the command line.\n",
    "\n",
    "1. Activate the environment by running the command ```conda activate data_analytics```.\n",
    "\n",
    "1. Make the kernel available in Jupyter: ```python -m ipykernel install --user --name=data_analytics```.\n",
    "\n",
    "1. Relaunch Jupyter: shutdown any running instances, and then type ```jupyter lab``` or ```jupyter notebook``` into your command line, depending on whether you prefer the full Jupyter lab development environment, or the simpler Jupyter notebook.\n",
    "\n",
    "1. Find this notebook and open it up again.\n",
    "\n",
    "1. Go to the top menu and change the kernel: click on 'Kernel'--> 'Change kernel' --> data_analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "annoying-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Use HuggingFace's datasets library to access the financial_phrasebank dataset\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f4a2e7-8a1f-4fed-a323-1b82d7080e03",
   "metadata": {},
   "source": [
    "# Financial Phrasebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8502fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset financial_phrasebank (/Users/es1595/.cache/huggingface/datasets/financial_phrasebank/sentences_50agree/1.0.0/a6d468761d4e0c8ae215c77367e1092bead39deb08fbf4bffd7c0a6991febbf0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "918ee1d714dc44af84b00a25c5955777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is a dictionary with two splits: \n",
      "\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label'],\n",
      "        num_rows: 4846\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# The financial_phrasebank dataset is available in four variations. It has no predefined train/validation/test splits.\n",
    "# Each data point was annotated by 5-8 people, then their annotations were combined. \n",
    "# Each variation of the dataset contains examples with different levels of agreement. \n",
    "# Let's use the one containing all data points where at least 50% of the annotators agreed on the label.\n",
    "dataset = load_dataset(\n",
    "    \"financial_phrasebank\", \n",
    "    'sentences_50agree' # Select variation of the dataset\n",
    ")\n",
    "\n",
    "print(f'The dataset is a dictionary with two splits: \\n\\n{dataset}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c8f839-6844-47d6-8997-bc076489fa9d",
   "metadata": {},
   "source": [
    "Let's create a test split, which we can hold out until we have tuned our method(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca009bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split test data from training data\n",
    "train_sentences, test_sentences, train_labels, test_labels = train_test_split(\n",
    "    dataset[\"train\"]['sentence'], \n",
    "    dataset[\"train\"]['label'], \n",
    "    test_size=0.2, \n",
    "    stratify=dataset[\"train\"]['label']  # make sure the same proportion of labels is in the test set and training set\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b3af369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many instances in the train dataset? \n",
      "\n",
      "3876\n",
      "\n",
      "What does one instance look like? \n",
      "\n",
      "Net sales of Kyro 's main business area , Glaston Technologies , a manufacturer of glass processing machines , decreased to EUR 161.5 mn from EUR 164.1 mn in January-September 2005 .\n"
     ]
    }
   ],
   "source": [
    "# label 0 = negative, 1 = neutral, 2 = positive\n",
    "print(f'How many instances in the train dataset? \\n\\n{len(train_sentences)}')\n",
    "print('')\n",
    "print(f'What does one instance look like? \\n\\n{train_sentences[234]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58ea906-8488-43a9-90dd-94695d700d19",
   "metadata": {},
   "source": [
    "It may also be necessary to create a _validation_ set (also called 'development' set or 'devset'). The validation set can be used to compute performance of your model when tuning hyperparameters,  optimising combinations of features, or looking at the errors your model makes before improving it. This allows you to hold out the test set to give a fair evaluation of the model and how well it generalises to new examples. This avoids tuning the model to specifso it gets good performance on the test set examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c873dbe-b6a6-41ed-9f03-b52a5e0a85c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_sentences, train_labels, test_size=0.25, stratify=train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "57d10fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many instances in the validation dataset? \n",
      "\n",
      "969\n",
      "\n",
      "How many instances in the test dataset? \n",
      "\n",
      "970\n"
     ]
    }
   ],
   "source": [
    "print(f'How many instances in the validation dataset? \\n\\n{len(val_sentences)}\\n')\n",
    "print(f'How many instances in the test dataset? \\n\\n{len(test_sentences)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2ee115-d861-43eb-8006-f4728256c5c4",
   "metadata": {},
   "source": [
    "# SEC Filings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d151a51-ff84-4583-b58b-87d9bbac18f9",
   "metadata": {},
   "source": [
    "The data is provided in zipped text files. Unzip the file and place the 'SEC-filings' directory into the './data' directory. \n",
    "\n",
    "A related dataset in similar format is the conll2003 dataset, available from [HuggingFace](https://huggingface.co/datasets/conll2003). This could be used as a source of additional training data, but note it contains a different kind of text (different 'domain'). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "642cdd01-b294-4f81-be43-b3ee23e8c090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def read_sec_filings(split):\n",
    "    # Use this function to load the SEC filings data from text files\n",
    "    \n",
    "    if split == 'train':\n",
    "        with open('./data/SEC-filings/train/FIN5.txt') as fp:\n",
    "            lines = fp.readlines()\n",
    "    else:\n",
    "        with open('./data/SEC-filings/test/FIN3.txt') as fp:\n",
    "            lines = fp.readlines()\n",
    "   \n",
    "    # store the tokens and labels for all sentences\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    # the tokens and labels for the current sentence\n",
    "    current_sen = []\n",
    "    current_labels = []\n",
    "\n",
    "    for i in range(2, len(lines)):\n",
    "        # print(f'This is line {i}')\n",
    "        # print(lines[i])\n",
    "\n",
    "        if len(lines[i]) > 1:  # Line with some data on: The data consists of tokens and tags.\n",
    "            data = re.split(' ', lines[i])  # tokenise the line\n",
    "            # print(data)\n",
    "            current_sen.append(data[0])  # append the token \n",
    "            \n",
    "            # data[1] contains POS tags -- you can also use these in your model.\n",
    "            \n",
    "            current_labels.append(data[3].strip())  # append the NER tag\n",
    "        elif len(current_sen) > 1:  # this marks the end of a sentence\n",
    "            # end of sentence\n",
    "            sentences.append(current_sen)  # save the tokens for this sentence\n",
    "            current_sen = []  # reset\n",
    "\n",
    "            labels.append(current_labels)  # save the tags for this sentence\n",
    "            current_labels = []\n",
    "\n",
    "    if len(current_sen) > 1:  # save the last sentence\n",
    "        sentences.append(current_sen)\n",
    "        labels.append(current_labels)\n",
    "    \n",
    "    print(f'Number of sentences loaded = {len(sentences)}')\n",
    "    print(f'Number of unique labels: {np.unique(np.concatenate(labels))}')\n",
    "                                      \n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d887df3-fd4b-448c-8545-01ff6c874665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the original training set: \n",
      "Number of sentences loaded = 1152\n",
      "Number of unique labels: ['I-LOC' 'I-MISC' 'I-ORG' 'I-PER' 'O']\n",
      "\n",
      "Loading the test set: \n",
      "Number of sentences loaded = 303\n",
      "Number of unique labels: ['I-LOC' 'I-MISC' 'I-ORG' 'I-PER' 'O']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('Loading the original training set: ')\n",
    "sentences_ner, labels_ner = read_sec_filings('train')\n",
    "\n",
    "print('\\nLoading the test set: ')\n",
    "test_sentences_ner, test_labels_ner = read_sec_filings('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4643a66d-d58b-4f3a-88ab-cb4963e75fe0",
   "metadata": {},
   "source": [
    "As before, we could create a validation split from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b753112-467c-42e0-8f22-3e733da33c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences = 921\n",
      "Number of validation sentences = 231\n"
     ]
    }
   ],
   "source": [
    "train_sentences_ner, val_sentences_ner, train_labels_ner, val_labels_ner = train_test_split(\n",
    "    sentences_ner, \n",
    "    labels_ner, \n",
    "    test_size=0.2,\n",
    "    # stratify=labels_ner  # there are too few examples of some classes to stratify\n",
    ")\n",
    "\n",
    "print(f'Number of training sentences = {len(train_sentences_ner)}')\n",
    "print(f'Number of validation sentences = {len(val_sentences_ner)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8186380-3995-4718-a24b-fd15182b274a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analytics",
   "language": "python",
   "name": "data_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
